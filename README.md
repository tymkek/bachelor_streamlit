# Evaluation of human knowledge with LLM and human feedback - Bachelor Degree Project

## Introduction

This repository consist of two main parts - *streamlit app* and *evaluation class*.

# Streamlit app

App made especially to collect data from users, in a form of answers to given set of questions, and evaluations to different answers based on the same question set.

## App components

## home_page

Shown to user as the starting page. Previews experiment rules. By going to the next page, user accepts to participate.

## choose_page

Short summaries (author, title, topics, reading time) of four texts are being shown. User chooses one to later read and answer questions that were generated by LLM in a specific to that article way.

## text_page

Text with a countdown. After the time is finished, user is redirected to question page.

## question_page

Five open questions. Answers are stored in JSON format.

## rate_page

Five answers created by previous users, with option to evaluate each on a scale {"very bad", "bad", "average", "good", "excellent"}.

# Evaluation class

Designed for a LLM, to evaluate user knowledge, based on given text, question set and answer set.



